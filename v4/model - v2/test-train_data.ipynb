{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"./2018-E-c-En-train.csv\", sep=';')\n",
    "\n",
    "not_chosen_columns = ['ID', 'Tweet']\n",
    "label_columns = [col for col in df_test.columns if col not in not_chosen_columns]\n",
    "\n",
    "df_labels_test = df_test[label_columns]\n",
    "\n",
    "list_labels_test = df_labels_test.values.tolist()\n",
    "\n",
    "test_texts = df_test['Tweet'].tolist()\n",
    "test_labels = list_labels_test\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"saved_model_try_6\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"saved_model_try_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# Process each text\n",
    "for text in test_texts:\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits)  # Apply sigmoid to convert logits to probabilities\n",
    "    probabilities = probabilities.squeeze(0)\n",
    "\n",
    "    threshold = 0.5\n",
    "    predicted_labels = (probabilities > threshold).long()\n",
    "\n",
    "    all_predictions.append(predicted_labels) #Değiştir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_n = torch.tensor(test_labels).to(device)\n",
    "all_predictions_n = torch.stack(all_predictions).to(true_labels_n.device)\n",
    "\n",
    "emotions = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\", \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n",
    "\n",
    "correct = (all_predictions_n == true_labels_n).float().sum().item()  # Count correct predictions\n",
    "total = torch.numel(true_labels_n)  # Total number of labels\n",
    "accuracy = correct / total  # Compute accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn\n",
    "predicted_labels_np = all_predictions_n.cpu().numpy()\n",
    "true_labels_np = true_labels_n.cpu().numpy()\n",
    "\n",
    "for i in range(true_labels_np.shape[1]):  # Iterate through each of the 11 labels\n",
    "    precision = precision_score(true_labels_np[:, i], predicted_labels_np[:, i], zero_division=0)\n",
    "    recall = recall_score(true_labels_np[:, i], predicted_labels_np[:, i], zero_division=0)\n",
    "    f1 = f1_score(true_labels_np[:, i], predicted_labels_np[:, i], zero_division=0)\n",
    "    print(f\"Label - {emotions[i]}: Precision={precision:.4f}, Recall={recall:.4f}, F1-score={f1:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(true_labels_np[:, i], predicted_labels_np[:, i])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix for Label {emotions[i]}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "f1 = f1_score(true_labels_np, predicted_labels_np, average=\"macro\")\n",
    "precision = precision_score(true_labels_np, predicted_labels_np, average=\"macro\")\n",
    "recall = recall_score(true_labels_np, predicted_labels_np, average=\"macro\")\n",
    "\n",
    "print(f\"\\nMacro - F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "f1 = f1_score(true_labels_np, predicted_labels_np, average=\"micro\")\n",
    "precision = precision_score(true_labels_np, predicted_labels_np, average=\"micro\")\n",
    "recall = recall_score(true_labels_np, predicted_labels_np, average=\"micro\")\n",
    "\n",
    "print(f\"Micro - F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "f1 = f1_score(true_labels_np, predicted_labels_np, average=\"weighted\")\n",
    "precision = precision_score(true_labels_np, predicted_labels_np, average=\"weighted\")\n",
    "recall = recall_score(true_labels_np, predicted_labels_np, average=\"weighted\")\n",
    "\n",
    "print(f\"Weighted - F1-score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
